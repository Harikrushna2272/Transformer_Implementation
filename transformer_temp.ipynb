{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class inputEmbedding(nn.Module):\n",
    "    def __init__(self, d_model:int, vocab_size:int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class positionalEmbedding(nn.Module):\n",
    "    # The -> None syntax is a type hint, which provides information about the expected return # type of the function. In this case, it suggests that the function does not return anything\n",
    "    def __init__(self, d_model, seq_len, dropout) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "\n",
    "        # Create a position index [0, 1, ..., max_len - 1]\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        # Calculate div_term using exponential decay based on embedding_dim\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        # Fill the positional encoding matrix\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # Apply sine to even indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # Apply cosine to odd indices\n",
    "\n",
    "        # Add a batch dimension for compatibility\n",
    "        pe = pe.unsqueeze(0)  # Shape: [1, max_len, embedding_dim]\n",
    "\n",
    "        # Register as a buffer (not a trainable parameter)\n",
    "        # The purpose of registering a tensor as a buffer is to make it accessible to the model, while ensuring that it is not treated as a trainable parameter. This can be useful for storing tensors that are used during the forward pass of the model but don't need to be updated during training\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add positional encoding (broadcasting over batch size)\n",
    "        x = x + (self.pe[:, :x.size(1) :]).requires_grad_(False)\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class layerNormalization(nn.Module):\n",
    "    def __init__(self, esp:float = 10**-6):\n",
    "        super().__init()\n",
    "        self.esp = esp\n",
    "        self.alpha = nn.Parameter(torch.ones(1)) # will Multiplied\n",
    "        self.beta = nn.Parameter(torch.zeros(1)) # will Added\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean = torch.mean(x, dim = -1, keepdim=True) # by keepdim dim will not deduct\n",
    "        std = torch.std(x, dim = -1, keepdim=True) # by keepdim dim will not deduct\n",
    "        eps = self.esp\n",
    "        return self.alpha * (x - mean) / (std + eps) + self.beta\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class feedForwardBlock(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout):\n",
    "        super().__init__\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.dropout(torch.relu(self.fc1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class multiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, h, dropout) -> None:\n",
    "        super().__init__()  \n",
    "        self.h = h  \n",
    "        self.d_model = d_model\n",
    "        assert d_model % h == 0, 'd_model is not divisible by zero'\n",
    "        d_k = d_model // h\n",
    "\n",
    "        self.w_q = nn.Linear(d_model, d_model) # wq\n",
    "        self.w_k = nn.Linear(d_model, d_model) # wk\n",
    "        self.w_v = nn.Linear(d_model, d_model) # wv\n",
    "        \n",
    "        self.w_o = nn.Linear(d_model, d_model) # wo\n",
    "        self.droput = nn.Dropout(dropout)\n",
    "\n",
    "    @ staticmethod\n",
    "    def attention(query, key, value, mask, dropout):\n",
    "        d_k = query.shape[-1]\n",
    "\n",
    "        # (batch_size, h, seq_len, d_model) -> (batch_size, h, seq_len, seq_len)\n",
    "        # matrix multiplication.\n",
    "        attention_score = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            attention_score.masked_fill_(mask == 0, -1e9)\n",
    "        attention_score = attention_score.softmax(dim=-1) # (batch, h, seq_len, seq_len)\n",
    "        if dropout is not None:\n",
    "            attention_score = dropout(attention_score)\n",
    "        \n",
    "        return (attention_score @ value), attention_score\n",
    "\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        query = self.w_q(q) # (batch_size, seq_len, d_model) -> (batch_size, seq_len, d_model)\n",
    "        key = self.w_k(k) # (batch_size, seq_len, d_model) -> (batch_size, seq_len, d_model)\n",
    "        value = self.w_v(v) # (batch_size, seq_len, d_model) -> (batch_size, seq_len, d_model)\n",
    "\n",
    "        # (batch_size, seq_len, d_model) -> (batch, seq_len, h, d_k) -> (batch_size, h, seq_len, d_model)\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "\n",
    "        x, self.attention_score = multiHeadAttention.attention(query, key, value, mask, self.dropout)\n",
    "\n",
    "        # (batch_size, h, seq_len, d_k) -> (batch_size, seq_len, h, d_model) -> (batch_size, seq_len, d_model)\n",
    "        x = x.transpose(1,2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "\n",
    "        return self.w_o(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- assert: The assert keyword is used to check if the condition specified is true. If the condition is false, the assert statement will raise an AssertionError exception.\n",
    "\n",
    "'d_model is not divisible by zero': This is the error message that will be displayed if the assertion fails, i.e., if d_model is not divisible by h without a remainder. -->\n",
    "\n",
    "<!-- The masked_fill_() method then replaces the values in the attention_score tensor at the positions where the mask is False (i.e., 0) with the value -1e9 (a large negative number).\n",
    "This effectively \"masks out\" the attention scores for the positions that should be ignored, by assigning them a very low value. -->\n",
    "\n",
    "<!-- contiguous() and non-contiguous memory:\n",
    "\n",
    "Tensors in PyTorch are stored in memory as a contiguous block of data. This means that the elements of the tensor are stored one after the other in memory.\n",
    "When you perform certain operations on a tensor, such as transposing or reshaping, the resulting tensor may not be stored contiguously in memory anymore.\n",
    "The contiguous() method ensures that the tensor is stored in a contiguous block of memory. This is important for certain operations, such as view(), which requires the tensor to be contiguous.\n",
    "If you try to call view() on a tensor that is not stored contiguously in memory, you will get a -->\n",
    "\n",
    "<!-- view(x.shape[0], -1, self.h * self.d_k) vs view(x.shape[0], -1):\n",
    "\n",
    "view(x.shape[0], -1, self.h * self.d_k) explicitly specifies the target shape, where the first dimension is the batch size (x.shape[0]), the last dimension is the product of the number of heads (self.h) and the dimensionality of each head (self.d_k), and the middle dimension is inferred as -1 (which means that PyTorch will calculate this dimension based on the total number of elements in the tensor and the other two specified dimensions).\n",
    "view(x.shape[0], -1) is a more concise version, where PyTorch will infer the second dimension based on the total number of elements in the tensor and the specified first dimension (batch size).\n",
    "Both versions will work, but the first one (view(x.shape[0], -1, self.h * self.d_k)) is more explicit and can help with readability and understanding the intended shape of the output tensor. -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class residualConnection(nn.Module):\n",
    "    def __init__(self, dropout);\n",
    "        super().__init__\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = layerNormalization() \n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoderBlock(nn.Module):\n",
    "    def __init__(self, self_attention_block : multiHeadAttention, feed_forward_block : feedForwardBlock, dropout):\n",
    "        super().__init__()\n",
    "        self.self_attention_block = multiHeadAttention\n",
    "        self.feed_forward_block = feedForwardBlock\n",
    "        self.residual_connection = nn.ModuleList([ResidualConnection(dropout)])\n",
    "\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        x = self.residual_connection[0](x, lambda x : self.multiHeadAttention(x, x, x, src_mask))\n",
    "        x = self.residual_connection[1](x, feedForwardBlock)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "    def __init__(self, layers : nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = layerNormalization\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoderBlock(nn.Module):\n",
    "    def _init__(self, self_attention : multiHeadAttention, cross_attnetin : multiHeadAttention, feed_forward_block : feedForwardBlock, droput):\n",
    "        super().__init__()\n",
    "        self.attention_block = seld_attention\n",
    "        self.cross_attention_block = cross_attention\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([residualConnection(dropout) for _ in range(3)])\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        x = self.residual_connections[0](x, lambda x : self.self_attention_block(x, x, x, tgt_mask))\n",
    "        x = self.residual_connections[0](x, lambda x : self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
    "        x = self.residual_connections[1](x, feed_forward_block)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- src_mask: A mask to prevent the decoder from attending to padding tokens in the encoder output.\n",
    "tgt_mask: A mask to prevent the decoder from attending to future tokens during self-attention. -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoder(nn.Module):\n",
    "    def __init__(self, layers : nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = layerNormalization\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        return self.norm(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
