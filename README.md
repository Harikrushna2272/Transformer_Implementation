# Transformer from Scratch using PyTorch üöÄ  

This repository contains a **Transformer model implemented from scratch using PyTorch**. The implementation follows the original paper *"Attention Is All You Need"* by Vaswani et al. and includes **multi-head self-attention, positional encoding, and feedforward layers**.  

## ‚ú® Features  

- **Full Transformer Architecture** (Encoder & Decoder)  
- **Implemented using PyTorch** (No external libraries like Hugging Face)  
- **Custom Positional Encoding**  
- **Multi-Head Self-Attention Mechanism**  
- **Feedforward Networks & Layer Normalization**  
- **Training Pipeline for NLP Tasks**  



## üõ†Ô∏è Installation  
To run this implementation, install the required dependencies:  
```bash
git clone https://github.com/Harikrushna2272/Transformer-PyTorch.git](https://github.com/Harikrushna2272/Transformer_Implementation
cd Transformer-PyTorch
pip install -r requirements.txt
# Transformer_Implementation
